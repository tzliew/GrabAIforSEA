{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab AI for SEA Challenge 1 (Traffic Management)\n",
    "This is a Time series problem. <br>This challenge is done in Python by method <b>Seasonal AutoRegressive Integrated Moving Average (SARIMA)</b> method.\n",
    "##### Name: Liam Liew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and reading required Data \n",
    "\n",
    "I use numpy and pandas to clean the data from training.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pmdarima\n",
    "import statsmodels\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_csv(\"training.csv\")\n",
    "full_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recalculate all the timestamp to 'time_order' attribute according to day and the timestamp. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 1 00:00 = 1, <br> \n",
    "Day 1 00:15 = 2, <br> \n",
    "Day 1 01:00 = 4, <br> \n",
    "Day 2 00:00 = 97,<br> \n",
    "Day 2 13:15 = 150<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the timestamp accordingly and calculate time_order\n",
    "full_data[\"time_order\"] = ((full_data[\"timestamp\"].str.split(\":\", n=1, expand=True)[0].astype(int) * 60 + \\\n",
    "                  full_data[\"timestamp\"].str.split(\":\", n=1, expand=True)[1].astype(int) + \\\n",
    "                 (full_data[\"day\"]-1)*24*60)/15).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the unnecessary columns\n",
    "full_data = full_data.drop([\"day\",\"timestamp\"], axis = 1)\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add another column 'count' to calculate how many data does each geohash6 location has in the training file\n",
    "full_data_indexed = full_data.set_index([\"geohash6\"])\n",
    "full_data_indexed['count'] = full_data_indexed.groupby('geohash6').count().iloc[:,0]\n",
    "\n",
    "# filter out the geohash6 that lack of data\n",
    "full_data_indexed = full_data_indexed[full_data_indexed['count']>=2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the data and separate it in a list with different location\n",
    "sorted_full_data_indexed = full_data_indexed.sort_values([\"count\",\"geohash6\",\"time_order\"], ascending = False)\n",
    "separated = [sorted_full_data_indexed.loc[geohash6,:] for geohash6 in sorted_full_data_indexed.index.unique()]\n",
    "\n",
    "# this list is for getting test data from dictionary purpose\n",
    "geohash6 = sorted_full_data_indexed.index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array to make sure that NULL value can be filled in the next few steps\n",
    "time_order_list = pd.DataFrame({'time_order' : np.arange(5856)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every location in the separated_data, merge the data with time_oreder_list so that we can fill the NULL value for some data\n",
    "# sort the time_order and delete the 'count' column after that \n",
    "separated = [pd.merge(separated[i], time_order_list, how='right', on='time_order').sort_values('time_order').drop([\"count\"], axis = 1).set_index(\"time_order\") for i in range(len(separated))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_data = {}\n",
    "for i,geo in enumerate(geohash6):\n",
    "    separated_data[geo] = separated[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Adfuller to test if the data is stationary for this time series problem\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "def test_stationarity(data_set):\n",
    "    \"\"\"\n",
    "    @parameter \n",
    "    data_set: data for a location including time_order and demand\n",
    "    \n",
    "    @return\n",
    "    this function will print the output for adfuller test and also the stationarity graph\n",
    "    \"\"\"\n",
    "    \n",
    "    #Determing rolling statistics\n",
    "    rolmean = data_set[\"demand\"].rolling(window=96).mean()\n",
    "    rolstd = data_set[\"demand\"].rolling(window=96).std()\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    orig = plt.plot(data_set, color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    #Perform Dickey-Fuller test:\n",
    "    print ('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(data_set[\"demand\"], autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print (dfoutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this challenge, we need to forecast 5 results of traffic demand in a particular location. Provided 14 consecutive days from the dataset, ending at timestamp T and predict T+1 to T+5, we need to check how many days do we need in advance to forecast the 5 results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this challenge, each day contains 96 different demand. <br/>Since we only need for 5 time intervals prediction with 14 days dataset, I separate out by different day to predict to increase the accuracy, meaning to say I will only use <b>previous N days</b> to predict based on the result we get.(Explantaion in word document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Use the correct function for testing (different from training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 5]\n",
    "\n",
    "def grabData(data_set, start, testday):\n",
    "    \"\"\"\n",
    "    @parameter \n",
    "    data_set: data for a location including time_order and demand\n",
    "    start: random 'time_order' number that to start for the tranning set \n",
    "    testday: the day that used to predict the next 5 time interval\n",
    "    \n",
    "    @return\n",
    "    this will return a stationarity data that to put in SARIMA model\n",
    "    \"\"\"\n",
    "    test = data_set[start: start + testday*96]\n",
    "\n",
    "    test = test.reset_index(drop = True)\n",
    "\n",
    "    # fill the gap if there is any missing or NULL value and if first few values are empty then back fill\n",
    "    final_test = test.interpolate().fillna(method = 'bfill')\n",
    "    return final_test\n",
    "    \n",
    "def grabDataTesting(data_set, testday):\n",
    "    \"\"\"\n",
    "    @parameter \n",
    "    data_set: data for a location including time_order and demand\n",
    "    testday: the day that used to predict the next 5 time interval\n",
    "    \n",
    "    @return\n",
    "    this will return a stationarity data that to put in SARIMA model\n",
    "    \"\"\"\n",
    "    test = data_set[-96*testday:]\n",
    "\n",
    "    test = test.reset_index(drop = True)\n",
    "\n",
    "    # fill the gap if there is any missing or NULL value and if first few values are empty then back fill\n",
    "    final_test = test.interpolate().fillna(method = 'bfill')\n",
    "    return final_test\n",
    "\n",
    "def stationarity(final_test):\n",
    "    \"\"\"\n",
    "    @parameter \n",
    "    final_test: the data used to test and forecast the demand from T+1 to T+5 interval\n",
    "    \n",
    "    @return\n",
    "    this will return a stationarity data that to plot ACF and PACF Charts\n",
    "    \"\"\"\n",
    "\n",
    "    # two differencing here to make the seasonal traffic data become a stationary data before going for SARIMA model\n",
    "    # d = 1, D = 1 for SARIMA model\n",
    "    diff1 = final_test - final_test.shift()\n",
    "    stationarity = diff1 - diff1.shift(96)\n",
    "    plt.plot(stationarity)\n",
    "    stationarity = stationarity.dropna()\n",
    "    test_stationarity(stationarity)\n",
    "    return stationarity\n",
    "\n",
    "# 100 is the random number to start and 4 is the 4 continuos day after time_order 100, it is used to predict after that\n",
    "# comment out this when doing testing\n",
    "final_test = grabData(separated_data[geohash6[3]],100,4)\n",
    "stationarity = stationarity(final_test)\n",
    "\n",
    "# uncomment it when doing testing\n",
    "# final_test = grabDataTesting(separated_data[\"geohash6--Name\"])\n",
    "# stationarity = stationarity(final_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "As in this step, I will observe whether there's a spike down from one bar to another in both charts. <br><b>Auto Correlation</b> is to determine <b>q</b> and <b>Q</b> value. <br> <b>Partial Auto Correlation</b> is for <b>p</b> and <b>P</b> value. We use the number 96 below is because there are 96 data in a day and it is seasonal period.\n",
    "#### To observe the Auto correlation and Partial auto correlation graph for the SARIMA(p,d,q)(P,D,Q)96. \n",
    "This is usually will be a range or number and waiting to be tested out by SARIMAX model according to AIC number. <br> In this case, we will use python to roughly tell us what is the range to use for p,q, P and Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,10))\n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "\n",
    "def acfPacf(stationarity):\n",
    "    \"\"\"\n",
    "    @parameter \n",
    "    stationarity: stationarity data for a location including time_order and demand\n",
    "    \n",
    "    @return\n",
    "    it will plot two chars: ACF and PACF\n",
    "    \"\"\"    \n",
    "    lag_acf = acf(stationarity, nlags=100)\n",
    "    lag_pacf = pacf(stationarity, nlags=100, method='ols')\n",
    "\n",
    "    plt.subplot(211)\n",
    "    plt.bar(np.arange(len(lag_acf)),lag_acf)\n",
    "    plt.axhline(y=0,linestyle = \"--\", color = 'gray')\n",
    "    plt.axhline(y = -1.96/np.sqrt(len(stationarity)), linestyle = \"--\", color = 'gray')\n",
    "    plt.axhline(y = 1.96/np.sqrt(len(stationarity)), linestyle = \"--\", color = 'gray')\n",
    "    plt.title(\"Auto Correlation Bar Chart\")\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.bar(np.arange(len(lag_pacf)),lag_pacf)\n",
    "    plt.axhline(y=0,linestyle = \"--\", color = 'gray')\n",
    "    plt.axhline(y = -1.96/np.sqrt(len(stationarity)), linestyle = \"--\", color = 'gray')\n",
    "    plt.axhline(y = 1.96/np.sqrt(len(stationarity)), linestyle = \"--\", color = 'gray')\n",
    "    plt.title(\"Partial Auto Correlation Bar Chart\")\n",
    "    \n",
    "acfPacf(stationarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my testing case, it is easy to determine that <b>p</b> is somehow in <b>range[0,1,2]</b>, <b>P</b>, <b>q</b> and <b>Q</b>  is either <b>0</b> or <b>1</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip this for testing\n",
    "we found out SARIMAX (1,1,1)(1,1,0)96 OR SARIMAX (1,1,0)(1,1,0)96 is the one with lowest AIC, which is the best model to be used in this challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = []\n",
    "model_fit_list= []\n",
    "\n",
    "# this is for training purpose to test out which is the best model for SARIMA\n",
    "# in the testing we found out SARIMAX (1,1,1)(1,1,0)96 OR SARIMAX (1,1,0)(1,1,0)96\n",
    "for day in range(3,8):\n",
    "    print(day)\n",
    "    print(\"RESULTS:\")\n",
    "    p_list = [0,1,2]\n",
    "    P_list = q_list = Q_list = [0,1]\n",
    "\n",
    "    final_test = grabData(separated_data[geohash[3]],100,day)\n",
    "    \n",
    "    # 96 is a seasonal period which there are 96 data in a day\n",
    "    final_model = SARIMAX(final_test, order=(0,1,0), seasonal_order=(0,1,0,96))\n",
    "    final_model_fit = final_model.fit()\n",
    "    for p in p_list:\n",
    "        for P in P_list:\n",
    "            for q in q_list:\n",
    "                for Q in Q_list:\n",
    "                    if (p!=0 or q!=0 or P!=0 or Q!=0):\n",
    "                        model = SARIMAX(final_test, order=(p,1,q), seasonal_order=(P,1,Q,96))\n",
    "                        try:\n",
    "                            model_fit = model.fit()\n",
    "                            if final_model_fit.aic > model_fit.aic:\n",
    "                                final_model = model\n",
    "                            print( \"SARIMAX model: non-seasonal(\",p,\", 1, \",q,\") seasonal(\",P,\", 1, \",Q,\") AIC value: \", model_fit.aic)\n",
    "                        except:\n",
    "                            print( \"SARIMAX model: non-seasonal(\",p,\", 1, \",q,\") seasonal(\",P,\", 1, \",Q,\") AIC value: NAN\")\n",
    "\n",
    "\n",
    "    # model_list and model_fit_list will add the best model (lowest AIC value) for different number of day as input\n",
    "    model_list.append(final_model.fit())\n",
    "    model_fit_list.append(final_model_fit.forecast(5))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use this for testing\n",
    "For the previous step, we found out SARIMAX (1,1,1)(1,1,0)96 OR SARIMAX (1,1,0)(1,1,0)96. Thus we compare only this two model with the lowest AIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change geohash[3] to a geohash6 name and second parameter for previous consecutive day for testing \n",
    "# comment out this for testing\n",
    "final_test = grabData(separated_data[geohash6[3]],100,4)\n",
    "\n",
    "# ---------------------------uncomment this for testing-------------------\n",
    "# second parameter is suggested to be 4 continuos day to predict the next 5 interval\n",
    "# final_test = grabDataTesting(separated_data[geohash6[3]],4)\n",
    "\n",
    "# 96 is a seasonal period which there are 96 data in a day\n",
    "final_model1 = SARIMAX(final_test, order=(1,1,1), seasonal_order=(1,1,0,96))\n",
    "final_model2 = SARIMAX(final_test, order=(1,1,0), seasonal_order=(1,1,0,96))\n",
    "fit1 = fit2 = True\n",
    "\n",
    "try:\n",
    "    final_model_fit1 = final_model1.fit()\n",
    "except:\n",
    "    fit1 = False\n",
    "    print(\"SARIMA (1,1,1)(1,1,0,96) is NA, not enough data. (Add the number of the day to test)\")\n",
    "\n",
    "try:\n",
    "    final_model_fit2 = final_model2.fit()\n",
    "except:\n",
    "    fir2 = False\n",
    "    print(\"SARIMA (1,1,0)(1,1,0,96) is NA, not enough data. (Add the number of the day to test)\")\n",
    "\n",
    "    \n",
    "if fit1 and fit2:\n",
    "\n",
    "    if final_model_fit1.aic > final_model_fit2.aic:\n",
    "        final_model = final_model2\n",
    "        final_model_fit = final_model.fit()\n",
    "    else:\n",
    "        final_model = final_model1\n",
    "        final_model_fit = final_model.fit()\n",
    "\n",
    "elif fit1 and not fit2:\n",
    "    final_model = final_model1\n",
    "    final_model_fit = final_model.fit()\n",
    "\n",
    "else:\n",
    "    final_model = final_model2\n",
    "    final_model_fit = final_model.fit()\n",
    "        \n",
    "        \n",
    "final_demand = final_model_fit.forecast(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Prediction and the actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(final_demand)\n",
    "# comment out this for testing\n",
    "plt.plot(grabData(separated_data[geohash6[3]],100,5)[:-91])\n",
    "\n",
    "# uncomment this for testing\n",
    "# plt.plot(grabDataTesting(separated_data[geohash6[3]],4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_fit.plot_diagnostics(figsize=(15, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the RMSE based on the actual and predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out this for testing\n",
    "RMSE = np.sum((final_demand.as_matrix() - grabData(separated_data[geohash6[3]],100,5)[-96:-91].values.reshape(-1))**2)/len(final_demand)\n",
    "\n",
    "# uncomment this for testing\n",
    "# RMSE = np.sum((final_demand.as_matrix() - grabDataTesting(separated_data[geohash6[3]],4)[-96:-91].values.reshape(-1))**2)/len(final_demand)\n",
    "\n",
    "RMSE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
